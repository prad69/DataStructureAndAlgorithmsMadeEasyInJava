{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjFwhD450iLW8aAjpR1YHv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prad69/DataStructureAndAlgorithmsMadeEasyInJava/blob/master/DeepLearnign_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwAw-hI62sxA"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning\n",
        "\n",
        "Gradient\n",
        "A gradient is a mathematical concept that plays a central role in optimization, particularly in machine learning and deep learning. It is a vector that contains the partial derivatives of a function with respect to all of its input variables. In simpler terms, the gradient tells us the direction and magnitude of the steepest ascent of a function.\n",
        "\n",
        "Key Points About Gradients:\n",
        "\n",
        "Definition:\n",
        "For a function , the gradient is a vector of its partial derivatives with respect to each input variable:\n",
        "\n",
        "Each component of the gradient represents how much the function\n",
        "f\n",
        "f changes when one specific variable changes, while keeping the others constant.\n",
        "Geometric Interpretation:\n",
        "The gradient points in the direction of the steepest ascent of the function. In other words, if you move in the direction of the gradient, the function's value increases the fastest.\n",
        "Conversely, the negative gradient points in the direction of the steepest descent, which is useful for minimizing functions (e.g., in gradient descent).\n",
        "Role in Machine Learning:\n",
        "In machine learning, the function of interest is often the loss function (\n",
        "L\n",
        "L), which measures how well the model's predictions match the true labels.\n",
        "The gradient of the loss function with respect to the model's parameters (weights and biases) tells us how to adjust those parameters to reduce the loss.\n",
        "\n",
        "Gradient in Neural Networks:\n",
        "In neural networks, the gradient is computed for each parameter (weight and bias) using backpropagation.\n",
        "The gradient is then used by optimization algorithms like gradient descent to update the parameters and minimize the loss function.\n",
        "Why Gradients Matter:\n",
        "Gradients provide a way to efficiently optimize complex functions, such as the loss functions in machine learning models.\n",
        "By following the direction of the negative gradient, we can iteratively improve the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "- An artificial neuron is a mathematical\n",
        "function conceived as a model of\n",
        "biological neurons, a neural network .\n",
        "Artificial neurons are elementary units in\n",
        "an artificial neural network.\n",
        "\n",
        "-  ANN is made of three layers namely input\n",
        "layer, output layer, and hidden layers.\n",
        "\n",
        "- There must be a connection from the nodes\n",
        "in the input layer with the nodes in the\n",
        "hidden layer and from each hidden layer\n",
        "node with the nodes of the output layer.\n",
        "\n",
        "\n",
        "- Artificial Neural Network (ANN) is a\n",
        "combination of multi-layer\n",
        "neural network with neurons present at\n",
        "each layer.\n",
        "- A basic ANN architecture consists\n",
        "of computational units and links.\n",
        "- The links have different weights on\n",
        "themselves depending on the\n",
        "weightage of different connections\n",
        "across the network.\n",
        "\n",
        "\n",
        "# Single Layer Perceptron\n",
        "\n",
        "![text](https://miro.medium.com/v2/resize:fit:639/1*_Epn1FopggsgvwgyDA4o8w.png)\n",
        "\n",
        "\n",
        "# Multi Layer Perceptron\n",
        "\n",
        "![](https://media.datacamp.com/legacy/v1725638284/image_bd3b978959.png)\n",
        "\n",
        "\n",
        "\n",
        "# Forward Propagation\n",
        "- Forward propagation is where input\n",
        "data is fed through a network, in a\n",
        "forward direction, to generate an\n",
        "output. The data is accepted by hidden\n",
        "layers and processed, as per\n",
        "the activation function, and moves to\n",
        "the successive layer.\n",
        "\n",
        "- During forward propagation,\n",
        "pre-activation and activation take place\n",
        "at each hidden and output layer node\n",
        "of a neural network. The pre-activation\n",
        "function is the calculation of the\n",
        "weighted sum. The activation function\n",
        "is applied, based on the weighted sum,\n",
        "to make the neural network flow\n",
        "non-linearly using bias.\n",
        "\n",
        "- The Weights and Biases are initialised\n",
        "randomly\n",
        "\n",
        "# Backward Propagation\n",
        "\n",
        "Backpropagation is the essence of neural network\n",
        "training. It is the method of fine-tuning the weights of\n",
        "a neural network based on the error rate obtained in\n",
        "the previous epoch (i.e., iteration). Proper tuning of the\n",
        "weights allows you to reduce error rates and make the\n",
        "model reliable by increasing its generalization .\n",
        "\n",
        "Most prominent advantages of Backpropagation are:\n",
        "a. Backpropagation is fast, simple and easy to program\n",
        "b. It has no parameters to tune apart from the numbers of input\n",
        "c. It is a flexible method as it does not require prior knowledge about\n",
        "the network\n",
        "d. It is a standard method that generally works well\n",
        "e. It does not need any special mention of the features of the function\n",
        "to be learned.\n",
        "\n",
        "• The two main formulas in backpropagation are – Chain\n",
        "Rule and Weight Updates .\n",
        "\n",
        "\n",
        "**How Backpropagation Works**\n",
        "Forward Pass: Input data is propagated through the network to generate an output.\n",
        "Error Calculation: The difference between the predicted output and the actual target is computed5.\n",
        "Backward Pass: The error is propagated backwards through the network5.\n",
        "Weight Updates: Gradients are used to adjust the weights and biases of the network, typically using optimization algorithms like gradient descent\n",
        "\n",
        "\n",
        "\n",
        "# Chain Rule\n",
        "Chain Rule is an important rule used in the Back Propagation step\n",
        "of neural networks learning phase which helps to find gradients/derivat\n",
        "ives of composite functions (combination of two or more functions)\n",
        "with respect to different inputs.\n",
        "\n",
        "\n",
        "#Gradient Descent\n",
        "\n",
        "\n",
        "\n",
        "#Convergence criteria\n",
        "\n",
        "- Small gradient: If the magnitude of the gradient becomes smaller than a threshold value (usually a small\n",
        "positive number), the algorithm is terminated.\n",
        "\n",
        "- Small step size: If the step size becomes smaller than a threshold value (usually a small positive number),\n",
        "the algorithm is terminated.\n",
        "\n",
        "- Maximum number of iterations: If the number of iterations reaches a maximum value specified by the\n",
        "user, the algorithm is terminated.\n",
        "\n",
        "- Small change in cost function: If the change in the cost function between iterations becomes smaller than\n",
        "a threshold value (usually a small positive number), the algorithm is terminated.\n",
        "\n",
        "- Prespecified tolerance: If the difference between the current and previous estimates of the optimal\n",
        "parameters becomes smaller than a tolerance value specified by the user, the algorithm is terminated.\n",
        "\n",
        "\n",
        "\n",
        "#Activation Functions\n",
        "\n",
        "![](https://sebastianraschka.com/images/faq/activation-functions/activation-functions.png)\n",
        "\n",
        "\n",
        "#Loss Functions\n",
        "\n",
        "- The function we want to minimize or\n",
        "maximize is called the objective function\n",
        "or criterion. When we are minimizing it,\n",
        "we may also call it the cost function, loss\n",
        "function, or error function.\n",
        "- It is important, therefore, that the function\n",
        "faithfully represent our design goals. If\n",
        "we choose a poor error function and\n",
        "obtain unsatisfactory results, the fault is\n",
        "ours for badly specifying the goal of the\n",
        "search.\n",
        "\n",
        "![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e4713a4-565e-446f-bc3c-3e8bb790e5a1_2587x3291.png)\n",
        "\n",
        "\n",
        "\n",
        "#Optimizers\n",
        "\n"
      ],
      "metadata": {
        "id": "fONvrwav20oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hsaSa4l-Ci0N"
      }
    }
  ]
}